# -*- coding: utf-8 -*-
"""Data301-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d56RQxxUEgVfY0dDDm1oKpRRFqitsqwx
"""

#library and code setup
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip install -q pyspark
import pyspark, os
from pyspark import SparkConf, SparkContext
os.environ["PYSPARK_PYTHON"]="python3"
os.environ["JAVA_HOME"]="/usr/lib/jvm/java-8-openjdk-amd64/"

!pip install gdelt
!pip install newspaper3k

#start spark local server
import sys, os
from operator import add
import time

os.environ["PYSPARK_PYTHON"]="python3"

import pyspark
from pyspark import SparkConf, SparkContext

#connects our python driver to a local Spark JVM running on the Google Colab server virtual machine
try:
  conf = SparkConf().setMaster("local[*]").set("spark.executor.memory", "1g")
  sc = SparkContext(conf = conf)
except ValueError:
  #it's ok if the server is already started
  pass

def dbg(x):
  """ A helper function to print debugging information on RDDs """
  if isinstance(x, pyspark.RDD):
    print([(t[0], list(t[1]) if 
            isinstance(t[1], pyspark.resultiterable.ResultIterable) else t[1])
           if isinstance(t, tuple) else t
           for t in x.take(100)])
  else:
    print(x)

"""0) Set up GDELT and retrieve relevant files"""

from concurrent.futures import ProcessPoolExecutor
from datetime import date, timedelta
import pandas as pd
import gdelt
import os

# set up gdeltpyr for version 2
gd = gdelt.gdelt(version=2)

# multiprocess the query
e = ProcessPoolExecutor()


# generic functions to pull and write data to disk based on date
def get_filename(x):
  date = x.strftime('%Y%m%d')
  return "{}_gdeltdata.csv".format(date)

def intofile(filename):
    try:
        if not os.path.exists(filename):
          date = filename.split("_")[0]
          d = gd.Search(date, table='events',coverage=False) #not updata at 15mins
          d.to_csv(filename,encoding='utf-8',index=False)
    except:
        print("Error occurred")

# pull the data from gdelt into multi files; this may take a long time
dates = [get_filename(x) for x in pd.date_range('2020 Apr 1','2020 May 7')]

results = list(e.map(intofile, dates))

"""1) Upload files as RDD"""

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

data = sqlContext.read.option("header", "true").csv(dates)

"""2) Create set of countries to investigate"""

countries = {'NZL', 'AUS', 'ITA', 'JPN','IND', 'DEU', 'RUS', 'GBR'} #, 'USA', 'CHN', 'DNK', 'DEU', 'FRA', 'GRL', 'JPN', 'KOR', 'PRK', 'NOR', 'RUS', 'SWE', 'CHE', 'Ton', 'ZWE', 'ARG', 'BRA', 'PAK', 'IND', 'TUR', 'ITA'} 
#New Zealand, Australia, United states, China, Denmark, Germanu, France, Greenland, Japan, South Korea, North Korea, Norway, Russia, Sweden, Switzerland, Tonga, Zimbabwe,
#Argentina, Brazil, Pakistan, India, Turkey, Italy

"""3) Sort articles into countries of Origin"""

data = data.rdd.filter(lambda row: row['Actor1CountryCode'] in countries).map(lambda row: (row['Actor1CountryCode'], row['SOURCEURL']))
#convert data into tuple of the source country and the article url before grouping by source country to produce ("Country", ('Url1', URL2'.....))

"""4) Perform text analysis"""

#Words that can be associated with a positive or negative tone (more or less arbitrary)
positive_words = ['great', 'good', 'excellent', 'happy', 'fantastic', 'positive', 'outstanding', 'congratulate', 'congratulating', 'congratulation', 'ideal', 'fortunate', 'incredible',
                  'confident', 'fast', 'quick', 'success', 'successful', 'relief', 'relieved']
negative_words = ['horrible', 'disaster', 'bad', 'unhappy', 'negative', 'disaster', 'horrifying', 'unideal', 'undesirable', 'blame', 'blamed', 'mishandled', 'tragedy', 'mistreatment',
                  'misfortune', 'upset', 'unconfident', 'slow', 'fail', 'failure', 'failed', 'questionable', 'unaware', 'protest', 'protests']
words_of_interest = positive_words + negative_words

import numpy as np
import urllib.request
from newspaper import Article
import nltk
nltk.download('popular', quiet=True)

if not os.path.exists('articles'):
  os.mkdir('articles')

def write_wget(withid):
  country = withid[0][0]
  url = withid[0][1]
  id = withid[1]
  s = 'articles/' + str(id) + ".html " + url
  if not os.path.exists(os.getcwd()+'/'+s):
    print("wget " + s)
    os.system('wget -O ' + s)
  return (country, id)

data_zipped = data.zipWithUniqueId() #Give each article_obj an id for assigning it to a file
withids = data_zipped.map(lambda zipped: write_wget(zipped))

def output_keywords(id):
  try:
    filename = "articles/"+str(id)+".html"
    filenameurl = "file://"+os.getcwd()+'/'+filename
    article = Article(filenameurl)
    with open(filename, 'r') as f:
      s = f.read()
      article.set_html(s)
    article.parse()
    article.nlp()
    contains_covid = False #for filtering out articles not talking about covid
    words_present = []
    words = article.text.split()
    for word in words:
      word = word.lower()
      if word == 'covid-19' or word == 'coronavirus':
        contains_covid = True
      if (word in words_of_interest):
        words_present.append(word)
    if not contains_covid:
      return None
    return words_present
  except Exception as e:
    print("Exception: " + str(e))
    return None

data_counts = withids.map(lambda article_info: (article_info[0], output_keywords(article_info[1])))
dbg(data_counts)

data_grouped = data_counts.filter(lambda article: article[1] != None).groupByKey()
dbg(data_grouped)

"""We now have our baskets set up such that each country is paired with its corresponding list of word counts in their own respective buckets

5) Perform interest calculations for each country
"""

def interest_table_singles(articles):
  """"Turn list of word counts into a list of interest values"""
  table = []
  for word in words_of_interest:
    total = 0
    for article in articles:
      for present_word in article:
        if (present_word == word):
          total += 1
    table.append((word, total))
  
  for i in range(len(table)):
    table[i]= (table[i][0], table[i][1] / len(articles))
  return table
  
interests_singles = data_grouped.map(lambda country: (country[0], interest_table_singles(country[1])))
dbg(interests_singles)

def make_pairs(input):
  """makes pairs of every item in a list"""
  new_list = []
  for i in range(len(input)):
    for j in range(len(input)):
      if input[j] != input[i] and (input[i], input[j]) not in new_list:
        new_list.append((input[i], input[j]))

  return new_list

pairs = make_pairs(words_of_interest)

def interest_table_pairs(articles):
  """turn list of word counts into interest value of each pair of words"""

  new_pairs = []
  for pair in pairs:
    word1 = pair[0]
    word2 = pair[1]
    total = 0
    for article in articles:
      word1_present = False
      word2_present = False
      for present_word in article:
        if present_word == word1:
          word1_present = True
        elif present_word == word2:
          word2_present = True
        if word1_present and word2_present:
          total += 1
    new_pairs.append((pair, total))


  for i in range(len(new_pairs)):
    new_pairs[i] = (new_pairs[i][0], new_pairs[i][1] / len(articles))

  return new_pairs

interests_paired = data_grouped.map(lambda country: (country[0], interest_table_pairs(country[1])))
dbg(interests_paired)

"""6) Sort the countries in a table based on their Manhattan Distance"""

collected_pairs = interests_paired.collect()

collected = interests_singles.collect()

#split tables into positive and negative counts
pos_words= []
neg_words = []
for country in collected:
    pos_words.append((country[0], country[1][:len(positive_words)]))
    neg_words.append((country[0], country[1][len(positive_words):]))

"""All data is collected from the articles and now just needs to be output in order and analysed with the reported numbers for the respective countries."""

def manhatten_distance_counts(table, tone):
  """sort table according to stats sorting"""
  new_table = []
  marker = [(None, 0) for i in range(tone)]
  complete = len(table)
  while len(new_table) < complete:
    closest = (0, float('inf'))
    for i in range(len(table)):
      if table[i] not in new_table:
        distance = 0
        for count in range(tone):
          diff = max(table[i][1][count][1], marker[count][1]) - min(table[i][1][count][1], marker[count][1])
          distance += diff
        if distance < closest[1]:
          closest = (i, distance)
          marker = table[i][1]
    new_table.insert(0, table[closest[0]])

  
  return new_table

pos_words = manhatten_distance_counts(pos_words, len(positive_words))
neg_words = manhatten_distance_counts(neg_words, len(negative_words))
pairs = manhatten_distance_counts(collected_pairs, len(pairs))

print("Interests of positive words:")
for country in pos_words:
  print(country)

print("---------------------------------------")

print("Interests of negative words:")
for country in neg_words:
  print(country)

print("---------------------------------------")

print("Interests of word pairs:")
for country in pairs:
  print(country)

"""7) Finally, print the table with the countries reported number.
The data was manually collected from https://www.worldometers.info/coronavirus/?utm_campaign=homeAdvegas1?%22%20%5Cl%20%22countries on 16/05/20, and I will us the total cases; deaths; and tests per million statistics.
"""

#Use their CAMEO codes to map the statistics to the countries on the table.
#The stats listed are cases, deaths, and tests per/million respectively 
#statistics = [('PRK', [0, 0, 0]), ('NZL', [311, 4, 45002]), ('AUS', [276, 4, 38639]), ('USA', [4487, 268, 33513]), ('CHN', [58, 3, 0]), ('DNK', [1864, 93, 63715]), ('DEU', [2098, 96, 37585]), ('FRA', [2751, 422, 21219]),
#              ('GRL', [194, 0, 28701]), ('JPN', [128, 6, 1843]), ('KOR', [215, 5, 14177]), ('NOR', [1518, 43, 3947]), ('RUS', [1801, 17, 43953]), 
#              ('SWE', [2894, 361, 17589]), ('CHE', [3529, 217, 38659]), ('ZWE', [3, 0.3, 1717]), ('ARG', [166, 8, 2146]), ('BRA', [1028, 70, 3462]), ('PAK', [169, 4, 1563]),
#              ('IND', [62, 2, 1480]), ('TUR', [1738, 48, 18373]), ('ITA', [3702, 523, 47553])]

statistics = [('ITA', [3702, 523, 47553]),('NZL', [311, 4, 45002]), ('AUS', [276, 4, 38639]), ('JPN', [128, 6, 1843]), ('IND', [62, 2, 1480]), ('RUS', [1801, 17, 43953]), ('DEU', [2098, 96, 37585]), ('GBR', [3849, 544, 52065])]

def manhattan_distance_stats(table):
  """find manhattan distance between countries base on their reported numbers"""
  new_table = [table[0]]
  while len(new_table) < len(table):
    marker = new_table[-1][1]
    closest_country = (None, float('inf'))
    for i in range(len(table)):
      if table[i] not in new_table:
        distance = 0
        for w in range(3):
          difference = max(table[i][1][w], marker[w]) - min(table[i][1][w], marker[w])
          distance += difference
        if (distance < closest_country[1]):
          closest_country = (i, difference)

    new_table.insert(len(new_table), table[closest_country[0]])
  
  return new_table

sorted_stats = manhattan_distance_stats(statistics)

for line in sorted_stats:
  print(line)

cases = statistics
cases.sort(key=lambda stats: stats[1][0], reverse=True)
for country in cases:
  print(country)

deaths = statistics
deaths.sort(key=lambda stats: stats[1][1], reverse=True)
for country in deaths:
  print(country)

tests = statistics
tests.sort(key=lambda stats: stats[1][2], reverse=True)
for country in tests:
  print(country)